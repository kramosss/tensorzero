{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ba5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80407c",
   "metadata": {},
   "source": [
    "# Fireworks Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune open-source LLMs using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "We follow the Fireworks [docs](https://docs.fireworks.ai/fine-tuning/fine-tuning-via-api) on fine-tuning a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40542c99",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL`, `FIREWORKS_API_KEY`, and `FIREWORKS_ACCOUNT_ID` environment variable. See the `.env.example` file.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56877706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLICKHOUSE_URL = os.getenv(\"TENSORZERO_CLICKHOUSE_URL\")\n",
    "FIREWORKS_API_KEY = os.getenv(\"FIREWORKS_API_KEY\")\n",
    "account_id = os.getenv(\"FIREWORKS_ACCOUNT_ID\")\n",
    "\n",
    "assert CLICKHOUSE_URL is not None, \"TENSORZERO_CLICKHOUSE_URL is not set\"\n",
    "assert FIREWORKS_API_KEY is not None, \"FIREWORKS_API_KEY is not set\"\n",
    "assert account_id is not None, \"FIREWORKS_ACCOUNT_ID is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"exact_match\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning (for Fireworks, NUM_EPOCHS * MAX_SAMPLES should be <= 3,000,000)\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://docs.fireworks.ai/fine-tuning/fine-tuning-models#supported-base-models)\n",
    "MODEL_NAME = \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n",
    "\n",
    "# At the time of writing, Fireworks does not support tool call content blocks in assistant messages. Or the tool role.\n",
    "# We will drop these invalid messages from the dataset by default.\n",
    "# You can set this to False to keep the invalid messages in the dataset.\n",
    "DROP_INVALID_MESSAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import warnings\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "import toml\n",
    "from IPython.display import clear_output\n",
    "from tensorzero import (\n",
    "    FloatMetricFilter,\n",
    "    RenderedSample,\n",
    "    TensorZeroGateway,\n",
    ")\n",
    "from tensorzero.util import uuid7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfd900",
   "metadata": {},
   "source": [
    "Initialize the embedded TensorZero client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=CLICKHOUSE_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62933c5a",
   "metadata": {},
   "source": [
    "Query for stored examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = FloatMetricFilter(\n",
    "    metric_name=METRIC_NAME, value=FLOAT_METRIC_THRESHOLD, comparison_operator=\">\"\n",
    ")\n",
    "# filters = BooleanMetricFilter(metric_name=METRIC_NAME, value=True)\n",
    "# You could also train on demonstrations by changing the output_source to \"demonstration\"\n",
    "stored_samples = t0.experimental_list_inferences(\n",
    "    function_name=FUNCTION_NAME,\n",
    "    filters=filters,\n",
    "    output_source=\"inference\",\n",
    "    limit=MAX_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe6727",
   "metadata": {},
   "source": [
    "Template the data using the variant we chose above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab26701",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_samples = t0.experimental_render_samples(\n",
    "    stored_samples=stored_samples, variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011eb99",
   "metadata": {},
   "source": [
    "Convert the rendered samples to the format Fireworks expects. This is handled automatically with our built-in `experimental_launch_optimization` method but we do it explicitly here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_message(role: str) -> str:\n",
    "    return (\n",
    "        f\"Fireworks does not support multiple content blocks per message. \"\n",
    "        f\"We have chosen to concatenate the text across all content blocks for the message with role '{role}'. \"\n",
    "        f\"You may want to manually review this behavior.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def render_message(message) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message.role\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message.content:\n",
    "        if content_block.type not in [\"text\", \"raw_text\"] and DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "        if content_block.type == \"text\":\n",
    "            parsed_content = content_block.text\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block.type == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block.value})\n",
    "        elif content_block.type == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block.text}</think>\"}\n",
    "            )\n",
    "        elif (\n",
    "            content_block.type == \"tool_call\"\n",
    "            and role == \"assistant\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif (\n",
    "            content_block.type == \"tool_result\"\n",
    "            and role == \"user\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool results in user messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            if len(content) > 1:\n",
    "                warnings.warn(warning_message(role), UserWarning)\n",
    "            role_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(output) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in output:\n",
    "        if content_block.type != \"text\" and DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "        if content_block.type == \"text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block.text})\n",
    "        elif content_block.type == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block.text}</think>\"}\n",
    "            )\n",
    "        elif content_block.type == \"tool_call\" and not DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block.type}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        if len(content) > 1:\n",
    "            warnings.warn(warning_message(\"assistant\"), UserWarning)\n",
    "        output_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def rendered_sample_to_fireworks(sample: RenderedSample) -> List[Dict[str, Any]]:\n",
    "    function_input = sample.input\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.system\n",
    "    if system:\n",
    "        rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input.messages:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(rendered_message)\n",
    "\n",
    "    # Add the output to the messages\n",
    "    rendered_output = render_output(sample.output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return rendered_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ae895",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireworks_samples = []\n",
    "for sample in rendered_samples:\n",
    "    rendered_sample = rendered_sample_to_fireworks(sample)\n",
    "    if rendered_sample is not None:\n",
    "        fireworks_samples.append(rendered_sample)\n",
    "\n",
    "print(f\"Found {len(fireworks_samples)} samples to fine-tune on\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef81c49",
   "metadata": {},
   "source": [
    "We'll write the conversational messages to a temporary file for the Fireworks API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = f\"t0-{uuid7()}\"\n",
    "api_base = \"https://api.fireworks.ai/v1\"\n",
    "base_headers = {\"Authorization\": f\"Bearer {FIREWORKS_API_KEY}\"}\n",
    "json_headers = base_headers.copy()\n",
    "json_headers.update({\"Content-Type\": \"application/json\"})\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\") as f:\n",
    "    for row in fireworks_samples:\n",
    "        f.write((json.dumps(row) + \"\\n\").encode(\"utf-8\"))\n",
    "    create_record_url = f\"{api_base}/accounts/{account_id}/datasets/\"\n",
    "\n",
    "    # Create dataset\n",
    "    create_record_result = requests.post(\n",
    "        create_record_url,\n",
    "        json={\n",
    "            \"datasetId\": dataset_id,\n",
    "            \"dataset\": {\n",
    "                \"displayName\": dataset_id,\n",
    "                \"format\": \"CHAT\",\n",
    "                \"exampleCount\": len(fireworks_samples),\n",
    "            },\n",
    "        },\n",
    "        headers=json_headers,\n",
    "    )\n",
    "    print(create_record_result)\n",
    "    # Upload dataset\n",
    "    upload_file_url = f\"{api_base}/accounts/{account_id}/datasets/{dataset_id}:upload\"\n",
    "\n",
    "    with open(f.name, \"r\") as file:\n",
    "        dataset = {\"file\": file}\n",
    "        upload_file_result = requests.post(\n",
    "            upload_file_url, headers=base_headers, files=dataset\n",
    "        )\n",
    "        print(upload_file_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4676b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_state_url = f\"{api_base}/accounts/{account_id}/datasets/{dataset_id}\"\n",
    "result = requests.get(check_state_url, headers=base_headers)\n",
    "print(json.dumps(result.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35c520",
   "metadata": {},
   "source": [
    "Now we start the fine-tuning job. This cell will block until the job is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sft_url = f\"{api_base}/accounts/{account_id}/supervisedFineTuningJobs\"\n",
    "json_to_send = {\n",
    "    \"dataset\": f\"accounts/{account_id}/datasets/{dataset_id}\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "}\n",
    "if NUM_EPOCHS is not None:\n",
    "    json_to_send[\"epochs\"] = NUM_EPOCHS\n",
    "result = requests.post(url=create_sft_url, headers=json_headers, json=json_to_send)\n",
    "if result.status_code != 200:\n",
    "    print(json.dumps(result.json(), indent=2))\n",
    "else:\n",
    "    response = result.json()\n",
    "    print(json.dumps(response, indent=2))\n",
    "    job_id = response[\"name\"]\n",
    "    print(f\"job_id: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc867bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status_url = f\"{api_base}/{job_id}\"\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        result = requests.get(\n",
    "            url=get_job_status_url,\n",
    "            headers=base_headers,\n",
    "        )\n",
    "        response = result.json()\n",
    "        print(json.dumps(result.json(), indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    if response[\"state\"] == \"JOB_STATE_FAILED\":\n",
    "        raise ValueError(\"Fine-tuning job failed\")\n",
    "\n",
    "    if response[\"state\"] == \"JOB_STATE_COMPLETED\":\n",
    "        break\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0af65",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = response[\"outputModel\"]\n",
    "deploy_model_url = f\"{api_base}/accounts/{account_id}/deployedModels\"\n",
    "result = requests.post(\n",
    "    url=deploy_model_url,\n",
    "    headers=json_headers,\n",
    "    json={\n",
    "        \"model\": model_id,\n",
    "        \"default\": True,\n",
    "        \"serverless\": True,\n",
    "        \"public\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = model_id\n",
    "\n",
    "assert model_identifier\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1c44a",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3c3ec",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215b20b",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `max_tokens`, `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
