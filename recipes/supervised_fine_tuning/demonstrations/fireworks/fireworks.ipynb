{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e329f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfb279",
   "metadata": {},
   "source": [
    "# Fireworks Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune open-source LLMs using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "We follow the Fireworks [docs](https://docs.fireworks.ai/fine-tuning/fine-tuning-models) on fine-tuning a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1b06e",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://chuser:chpassword@localhost:8123/tensorzero\"`\n",
    "- You'll also need to [install](https://docs.fireworks.ai/tools-sdks/firectl/firectl) the CLI tool `firectl` on your machine and sign in with `firectl signin`. You can test that this all worked with `firectl whoami`.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd242ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CLICKHOUSE_URL = os.getenv(\"TENSORZERO_CLICKHOUSE_URL\")\n",
    "CLICKHOUSE_URL = \"http://chuser:chpassword@localhost:8123/tensorzero_ui_fixtures\"\n",
    "assert CLICKHOUSE_URL is not None, \"TENSORZERO_CLICKHOUSE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7f5c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# You can also fine-tune on demonstrations sent to TensorZero \n",
    "METRIC_NAME = \"jaccard_similarity\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning (for Fireworks, NUM_EPOCHS * MAX_SAMPLES should be <= 3,000,000)\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://docs.fireworks.ai/fine-tuning/fine-tuning-models#supported-base-models)\n",
    "MODEL_NAME = \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n",
    "\n",
    "# At the time of writing, Fireworks does not support tool call content blocks in assistant messages. Or the tool role.\n",
    "# We will drop these invalid messages from the dataset by default.\n",
    "# You can set this to False to keep the invalid messages in the dataset.\n",
    "DROP_INVALID_MESSAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e736b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tensorzero.util import uuid7\n",
    "from tensorzero import TensorZeroGateway, FloatMetricFilter, RenderedSample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210efac6",
   "metadata": {},
   "source": [
    "Initialize the embedded TensorZero client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1491cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=CLICKHOUSE_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f386f60",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aa18237",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c44cc0",
   "metadata": {},
   "source": [
    "Query inference data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "895d37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = FloatMetricFilter(metric_name=METRIC_NAME, value=FLOAT_METRIC_THRESHOLD, comparison_operator=\">\")\n",
    "stored_samples = t0.experimental_list_inferences(function_name=FUNCTION_NAME, \n",
    "                                                 filters=filters,\n",
    "                                                 output_source=\"inference\", # could also be \"demonstration\"\n",
    "                                                 limit=MAX_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34927e9",
   "metadata": {},
   "source": [
    "Template the data using the variant we chose above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c0108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_samples = t0.experimental_render_samples(stored_samples=stored_samples, variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3abc632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendered_samples[0].input.messages[0].role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dc545",
   "metadata": {},
   "source": [
    "Convert the rendered samples to the format Fireworks expects. This is handled automatically with our built-in `experimental_launch_optimization` method but we do it explicitly here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "189049ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_message(role: str) -> str:\n",
    "    return (\n",
    "        f\"Fireworks does not support multiple content blocks per message. \"\n",
    "        f\"We have chosen to concatenate the text across all content blocks for the message with role '{role}'. \"\n",
    "        f\"You may want to manually review this behavior.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def render_message(message) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message.role\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message.content:\n",
    "        if content_block.type not in [\"text\", \"raw_text\"] and DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "        if content_block.type == \"text\":\n",
    "            parsed_content = content_block.text\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block.type == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block.value})\n",
    "        elif content_block.type == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block.text}</think>\"}\n",
    "            )\n",
    "        elif (\n",
    "            content_block.type == \"tool_call\"\n",
    "            and role == \"assistant\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif (\n",
    "            content_block.type == \"tool_result\"\n",
    "            and role == \"user\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool results in user messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            if len(content) > 1:\n",
    "                warnings.warn(warning_message(role), UserWarning)\n",
    "            role_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in output:\n",
    "        if content_block.type != \"text\" and DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "        if content_block.type == \"text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block.text})\n",
    "        elif content_block.type == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block.text}</think>\"}\n",
    "                )\n",
    "        elif content_block.type == \"tool_call\" and not DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block.type}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        if len(content) > 1:\n",
    "            warnings.warn(warning_message(\"assistant\"), UserWarning)\n",
    "        output_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def rendered_sample_to_fireworks(sample: RenderedSample) -> List[Dict[str, Any]]:\n",
    "    function_input = sample.input\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.system\n",
    "    if system:\n",
    "        rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input.messages:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(rendered_message)\n",
    "\n",
    "    # Add the output to the messages\n",
    "    rendered_output = render_output(sample.output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return rendered_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e29eb457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69 samples to fine-tune on\n"
     ]
    }
   ],
   "source": [
    "fireworks_samples = []\n",
    "for sample in rendered_samples:\n",
    "    rendered_sample = rendered_sample_to_fireworks(sample)\n",
    "    if rendered_sample is not None:\n",
    "        fireworks_samples.append(rendered_sample)\n",
    "\n",
    "print(f\"Found {len(fireworks_samples)} samples to fine-tune on\")\n",
    "\n",
    "# We can now fine-tune on the samples\n",
    "\n",
    "# We can now fine-tune on the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87a461",
   "metadata": {},
   "source": [
    "We'll write the conversational messages to a temporary file for the Fireworks CLI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "974a8d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "dataset_id = f\"t0-{uuid7()}\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\") as f:\n",
    "    for sample in fireworks_samples:\n",
    "        f.write((json.dumps(sample) + \"\\n\").encode(\"utf-8\"))\n",
    "\n",
    "    dataset_path = f.name\n",
    "    result = subprocess.run(\n",
    "        [\"firectl\", \"create\", \"dataset\", dataset_id, dataset_path], capture_output=True\n",
    "    )\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4b7f26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run([\"firectl\", \"get\", \"dataset\", dataset_id], capture_output=True)\n",
    "print(result.stdout.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e6553b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\"/\")[-1].strip()\n",
    "    raise ValueError(\"Job ID not found in output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0387d",
   "metadata": {},
   "source": [
    "Now we start the fine-tuning job. This cell will block until the job is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18cf3ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command:  firectl create sftj --display-name tensorzero-ft-job-t0-0198230b-0139-7941-a41a-9627d174d363 --dataset t0-0198230b-0139-7941-a41a-9627d174d363 --base-model accounts/fireworks/models/llama-v3p1-8b-instruct --epochs 1\n",
      "2025/07/19 10:16:32 There are updates available.\n",
      "\tCurrent version: 1.4.3\n",
      "\tLatest version: 1.5.11\n",
      "\n",
      "\tTo upgrade to the latest version, run\n",
      "  \t$ sudo firectl upgrade\n",
      "\n",
      "\t\n",
      "2025/07/19 10:16:32 Failed to execute: unknown flag: --display-name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "command = [\n",
    "    \"firectl\",\n",
    "    \"create\",\n",
    "    \"sftj\",\n",
    "    \"--display-name\",\n",
    "    f\"tensorzero-ft-job-{dataset_id}\",\n",
    "    \"--dataset\",\n",
    "    dataset_id,\n",
    "    \"--base-model\",\n",
    "    MODEL_NAME,\n",
    "]\n",
    "\n",
    "if NUM_EPOCHS is not None:\n",
    "    command.append(\"--epochs\")\n",
    "    command.append(str(NUM_EPOCHS))\n",
    "\n",
    "print(\"Command: \", \" \".join(command))\n",
    "\n",
    "result = subprocess.run(command, capture_output=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(result.stderr.decode(\"utf-8\"))\n",
    "else:\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(stdout)\n",
    "    job_id = get_job_id(stdout)\n",
    "    print(f\"job_id: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5679ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'job_id' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stdout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState: JOB_STATE_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstdout\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning job failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState: JOB_STATE_COMPLETED\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stdout:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stdout' is not defined"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        command = [\"firectl\", \"get\", \"sftj\", job_id]\n",
    "        result = subprocess.run(command, capture_output=True)\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    if \"State: JOB_STATE_FAILED\" in stdout:\n",
    "        raise ValueError(\"Fine-tuning job failed\")\n",
    "\n",
    "    if \"State: JOB_STATE_COMPLETED\" in stdout:\n",
    "        break\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Output Model:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model ID not found in output\")\n",
    "\n",
    "\n",
    "model_id = get_model_id(stdout)\n",
    "\n",
    "assert model_id\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc47847",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc33b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\"firectl\", \"deploy\", model_id]\n",
    "print(\" \".join(command))\n",
    "result = subprocess.run(command, capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    print(result.stderr.decode(\"utf-8\"))\n",
    "else:\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_identifier(model_id: str) -> str:\n",
    "    command = [\"firectl\", \"get\", \"model\", model_id]\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model identifier not found in output\")\n",
    "\n",
    "\n",
    "model_identifier = get_model_identifier(model_id)\n",
    "\n",
    "assert model_identifier\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b0caf",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d37954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be76ed",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": model_identifier,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {model_identifier: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a1cd4",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `max_tokens`, `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
